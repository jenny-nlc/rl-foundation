\section{Problems and Existing Solutions}

% \subsection{low sample efficiency}
% WHY:
% * on-policy learning; train on each trajectory only once,
%   * recall: vanilla actor-critic methods are on-policy only
%   * on-policy leads to good convergence, why?

% SOLUTION:
% * off-policy learning (via replay buffer),
%   BUT
%   * may lead to bad convergence, why?
%   EG:
%   * reactor~\cite{Gruslys2018}: prioritized sequence replay
%   * qprop:  Taylor expansion of the critic as a control variate
%   * acer: 3 contrib:
%     truncated importance sampling with bias correction,
%     stochastic dueling network architectures, and
%     a new trust region policy optimization method.

% * use more advanced optimization techniques
% EG
% * acktr: kfac

% * soft-ac: max entropy actor critic

% \subsection{tradeoff bias and variance in estimating the gradient}
% AKA
% * overestimation on value estimate

% SOLUTION
% * xxx
%   EG:
%   * reactor: beta-leave-one-out policy gradient estimate
% * xxx2
%   EG:
%   * GAC: guide actor
% * favors underestimations
%   EG:
%   * TD3
% * dual critic

% \subsection{computational efficiency}
% AKA
% computation speed,
% time efficiency,
% training time

% WHY
% * applying natural grad

% SOLUTION
% * paralel (distributive nature)
%   EG:
%   * reactor
%     * distributional retrace

% CONCERN
% * gpu vs cpu computation

% \subsection{ease of implementation, deployment}
% AKA
% reproducibility
% * sensitive to hyperparameter settings

We focus on actor-critic approaches that are non-hierarchical and
consider fully-observable single task and single agent,
as well as both discrete and continuous action spaces.

\subsection{Low sample efficiency (high sample complexity)}
This problem stems from the fact that vanilla actor-critic methods are on-policy only.
On-policy learning leads to good convergence to the target policy at
the cost of low sample efficiency as it trains on each trajectory only once.\\

% Low sample effiency is also becuase policy grad use monte-carlo techniques
% that tend to have high variance, which leads to slow convergence

% low sample efficiency also due to model-free setting

\noindent
\textbf{Off-policy solutions:}\\
One straightforward solution for this is to train off-policy using a replay buffer.
This, however, results in bad convergence because the training data
do not come from the current policy, but from another behaviour policy.
Consequently, works in this line propose ideas that \emph{not only} benefit from
off-policy learning \emph{but also} (try to) maintain convergence.
For example:
Reactor~\cite{Gruslys2018},
Q-prop~\cite{GuLilGhaTurLev17},
SAC~\cite{abs-1801-01290},
ACER~\cite{WangBHMMKF16}.\\

% off-policy include those with model?

\noindent
\textbf{On-policy solutions:}\\
Other solutions for low sample officiency do not incorporate off-policy learning.
In other words, they stay on-policy.
We notice that the number of works using on-policy are fewer (compared to those using off-policy).
For example:
ACKTR~\cite{NIPS2017_7112},
A3C~\cite{pmlr-v48-mniha16}.

\subsection{Bias and variance tradeoff}
Actor-critic methods originate from policy-based methods in model-free setting that
wait until the end of an episode to estimate the policy gradient;
in model-based, we use Monte Carlo simulation.
Consequently, they have high variance.
The value from critic is aimed to reduce the variance and to accelerate learning;
although it introduces bias and an asymptotic dependence on the quality of the function approximation.
As a result, the intricacy between actor and critic can be encoded as the bias and variance tradeoff.
For example:
GAC~\cite{tangkaratt2018guide},
TD3~\cite{abs-1802-09477},
In Dual-AC~\cite{dai2018boosting}.

% major shortcoming of these algorithms, namely, the high variance of the gradient estimates.
% This problem may be traced to the fact that in most interesting cases, the time-average of
% the observed rewards is a high-variance (although unbiased) estimator of the true average
% reward, resulting in the sample-inefficiency of these algorithms.

% See, bayesian actor critic, GPS, GAE
lots work on policy-based try to reduce variance

\subsection{Low reproducibility}
The reproducibility issue in actor-critic approaches is naturally worse than in general RL~\cite{henderson2017reinforcement}.
This is because there are, at least, 2 neural networks in deep actor-critic RL;
one for actor, one for critic.
Although it is theoretically possible to use one network for both actor and critic,
in practice, especially for continuous action spaces, separate networks are superior~\cite{pmlr-v48-mniha16}.
This implies that there are more hyperparameters to tune.
For example, DDPG~\cite{LillicrapHPHETS15} requires 4 neural networks.
Moreover, its performance is known to be sensitive to hyperparameter settings~\cite{Duan:2016}.

There exist several initiatives for better reproducibility in terms of standardization.
In our opinion, the top-2 are OpenAI's baseline~\cite{baselines} and gym~\cite{1606.01540}.
The former provides high quality implementation of major actor-critic algorithms.
Whereas, the latter has a set of standardized test environments for both discrete and continuous action spaces.
We observe that the community progressively accepts these baseline and gym for benchmarking.
The progress, however, has been slowed down mainly due to technical choices of the deep-net backend,
either tensorflow (as in the baseline) or pytorch.
