\section{Problems}

\subsection{low sample efficiency}
AKA
low data efficiency
hi sample complexity

WHY:
* on-policy learning; train on each trajectory only once,
  * recall: vanilla actor-critic methods are on-policy only

SOLUTION:
* off-policy learning (via replay buffer),
  BUT
  * may lead to bad convergence
  EG:
  * reactor~\cite{Gruslys2018}
    * prioritized sequence replay

* use more advanced optimization techniques
EG
* acktr: kfac


\subsection{unguaranteed stability}
AKA
stable, good convergence

CONCERNS
* off-policy learning
* nonconvex optim in net
* nonlinear gn approx of net

\subsection{hi computational efficiency}
AKA
computation speed,
time efficiency,
training time

WHY
* applying natural grad

SOLUTION
* paralel (distributive nature)
  EG:
  * reactor
    * distributional retrace

CONCERN
* gpu vs cpu computation

\subsection{tradeoff bias and variance in estimating the gradient}
AKA
* overestimation on value estimate

SOLUTION
* xxx
  EG:
  * reactor
    * beta-leave-one-out policy gradient estimate
* xxx2
  EG:
  * GAC
    * guide actor
* favors underestimations
  EG:
  * TD3

\subsection{ease of implementation, deployment, hyperparams tuning}


