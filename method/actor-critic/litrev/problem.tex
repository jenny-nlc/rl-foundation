\section{Problems}

\subsection{low sample efficiency}
AKA
low data efficiency
hi sample complexity

WHY:
* on-policy learning; train on each trajectory only once,
  * recall: vanilla actor-critic methods are on-policy only

SOLUTION:
* off-policy learning (via replay buffer),
  EG:
  * reactor~\cite{Gruslys2018}
    * prioritized sequence replay

  BUT
  * may lead to bad convergence

\subsection{unguaranteed stability}
AKA
stable, good convergence

\subsection{hi computational efficiency}
AKA
computation speed,
time efficiency

SOLUTION
* paralel (distributive nature)
  EG:
  * reactor
    * distributional retrace

CONCERN
* gpu vs cpu computation

\subsection{tradeoff bias and variance in estimating the gradient}
AKA
* overestimation on value estimate

SOLUTION
* xxx
  EG:
  * reactor
    * beta-leave-one-out policy gradient estimate
* xxx2
  EG:
  * GAC
    * guide actor
* favors underestimations
  EG:
  * TD3

\subsection{ease of implementation, deployment, hyperparams tuning}


