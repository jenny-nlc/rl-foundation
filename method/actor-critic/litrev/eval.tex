\section{Evaluation}
% use reported values,
% eval metric

% Identify components

% * psi: the advantage that is put in gradient

% * optimization method: adam, rmsprop, sgd, kfac
%   * order: first, second

% * constraint in opt: KL-div

% * OFF - ON POLICY
%   off-policy learning: for sample efficiency at the cost of learning stability

% * ACTOR AND CRITIC NETS: shared, separated

% To ensure exploration in the policy
% * to use entropy regularization to ensure exploration in the policy,
% which is a common practice in policy gradient (Williams & Peng, 1991; Mnih et al., 2016).
% * to use KL-divergence as a constraint on how much deviation is permitted from a prior policy
% (Bagnell & Schneider, 2003; Peters et al., 2010; Schulman et al., 2015; Fox et al., 2015).

% \section{Taxonomy}

% \subsection{On- vs -off policy learning}
% On-policy:
% A3C~\cite{Mnih2016},
% ACKTR,
% natural AC,

% Off-policy:
% Reactor~\cite{Gruslys2018},
% SoftAC~\cite{Haarnoja2017},
% ACER~\cite{Wang2016},
% using experience replay buffer,

% \subsection{stoc vs det policy}

% \subsection{Discrete action spaces: Atari tasks}
% Atari game~\footnote{\url{https://gym.openai.com/envs/\#atari}}.

% Properties:
% \begin{itemize}
% \item discrete action spaces
% \item 57 games
% \end{itemize}

% Reactor: LSTM net

% \subsection{Continuous action space: Mujoco tasks}
% Mujoco tasks~\footnote{\url{https://gym.openai.com/envs/\#mujoco}}

% Properties:
% \begin{itemize}
% \item continuous action spaces
% \item 6 task: cartpole (1D), reacher (3D), cheetah (6D), fish (5D), walker (6D) and humanoid (21D)
% \end{itemize}

% \begin{table}[]
% \centering
% \caption{My caption}
% \label{my-label}

% \begin{tabular}{|l|l|l|l|}
% \hline
% Methods & Setup & Atari Tasks & Mujoco Tasks \\ \hline
% ACKTR   & s1    & a1          & m1           \\ \hline
% ACER    & s2    & a2          & m2           \\ \hline
% \end{tabular}

% \end{table}
