\section{Introduction}

One promising approach in RL is actor-critic.
Briefly, it takes advantage of both value- and policy-based RL.
One can view that actor-critic methods originate from policy-based RL, ie. REINFORCE.

The REINFORCE-with-baseline algorithm learns both a policy and a state-value function.
However, it does not belong to actor-critic becuase
its state-value function is used only as a baseline, not as a critic.
In other words, the value function is not used for bootstrapping
(updating the value estimate for a state from the estimated values of subsequent states),
but only as a baseline for the state whose estimate is being updated.
Such bootstrapping is useful to reduces variance and accelerates learning,
although it introduce bias and an asymptotic dependence on
the quality of the function approximation.

Figure~\ref{} shows REINFORCE-with-baseline and one-step vanilla actor-critic.
The latter is on-policy actor-critic.
TODO: iterate the diff.

Challenges include:
\begin{itemize}
\item data/sample efficiency,
\item stable, good convergence
\end{itemize}

The trend:
\begin{itemize}
\item deep neural networks as function approximators.
\item off-policy learning
\item scales to both continuous and discrete action spaces
\end{itemize}

Survey on actor-critic include:
\cite{6392457}
