\section{Introduction}

One promising approach in RL is actor-critic.
Briefly, it takes advantage of both value- and policy-based RL.
One can view that actor-critic methods originate from policy-based RL, ie. REINFORCE.

The REINFORCE-with-baseline algorithm learns both a policy and a state-value function.
However, it does not belong to actor-critic becuase
its state-value function is used only as a baseline, not as a critic.
In other words, the value function is not used for bootstrapping
(updating the value estimate for a state from the estimated values of subsequent states),
but only as a baseline for the state whose estimate is being updated.
Such bootstrapping is useful to reduces variance and accelerates learning,
although it introduce bias and an asymptotic dependence on
the quality of the function approximation.

Figure~\ref{} shows REINFORCE-with-baseline and one-step vanilla actor-critic.
The latter is on-policy actor-critic.
TODO: iterate the diff.

Problems include:
\begin{itemize}
\item data/sample efficiency,
    WHY:
    * on-policy learning; train on each trajectory only once,
    SOLUTION:
    * off-policy learning (via replay buffer),
      but this may lead to bad convergence
      eg: REACTOR~\cite{Gruslys2018}
\item stable, good convergence
\item computational efficiency, computation speed/time
    CONCERN
    * paralel: gpu vs cpu computation

\item ease of implementation, deployment, hyperparams tuning
\item tradeoff bias and variance in estimating the gradient
\end{itemize}

The trend:
\begin{itemize}
\item deep neural networks as
      function approximators and policy representation
\item scales to both continuous and discrete action spaces
\item natural policy gradient, instead of standard gradient
\end{itemize}

Survey on actor-critic include:
\cite{6392457}

% focus
% single agent
% non-hierarchy
% single task
% those with openai's (and or deepmind's) atari and mujoco tasks
% nonlinear fn approx, eg net
% fully observable

