@inbook{citeulike:14544227,
    address = {Chichester, UK},
    author = {Botev, Zdravko and Ridder, Ad},
    booktitle = {Wiley StatsRef: Statistics Reference Online},
    day = {14},
    doi = {10.1002/9781118445112.stat07975},
    editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
    isbn = {9781118445112},
    month = apr,
    pages = {1--6},
    posted-at = {2018-05-04 18:49:21},
    priority = {2},
    publisher = {John Wiley \& Sons, Ltd},
    title = {{Variance Reduction}},
    url = {http://dx.doi.org/10.1002/9781118445112.stat07975},
    year = {2017}
}

@misc{henderson2017reinforcement,
  author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  biburl = {https://www.bibsonomy.org/bibtex/2b5bd5f75948f959eac4a2bf2fce9ef42/achakraborty},
  description = {[1709.06560] Deep Reinforcement Learning that Matters},
  interhash = {6f4ef32093f8db0b16430338bda8a326},
  intrahash = {b5bd5f75948f959eac4a2bf2fce9ef42},
  keywords = {2017 arxiv deep-learning reinforcement-learning},
  note = {cite arxiv:1709.06560Comment: Accepted to the Thirthy-Second AAAI Conference On Artificial  Intelligence (AAAI), 2018},
  timestamp = {2018-02-07T04:57:21.000+0100},
  title = {Deep Reinforcement Learning that Matters},
  url = {http://arxiv.org/abs/1709.06560},
  year = 2017
}

@misc{baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@misc{1606.01540,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@inproceedings{Duan:2016,
 author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
 title = {Benchmarking Deep Reinforcement Learning for Continuous Control},
 booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
 series = {ICML'16},
 year = {2016},
 location = {New York, NY, USA},
 pages = {1329--1338},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3045390.3045531},
 acmid = {3045531},
 publisher = {JMLR.org},
}

@article{LillicrapHPHETS15,
  author    = {Timothy P. Lillicrap and
               Jonathan J. Hunt and
               Alexander Pritzel and
               Nicolas Heess and
               Tom Erez and
               Yuval Tassa and
               David Silver and
               Daan Wierstra},
  title     = {Continuous control with deep reinforcement learning},
  journal   = {CoRR},
  volume    = {abs/1509.02971},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.02971},
  archivePrefix = {arXiv},
  eprint    = {1509.02971},
  timestamp = {Wed, 07 Jun 2017 14:40:26 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LillicrapHPHETS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
dai2018boosting,
title={Boosting the Actor with Dual Critic},
author={Bo Dai and Albert Shaw and Niao He and Lihong Li and Le Song},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BkUp6GZRW},
}

@article{abs-1802-09477,
  author    = {Scott Fujimoto and
               Herke van Hoof and
               Dave Meger},
  title     = {Addressing Function Approximation Error in Actor-Critic Methods},
  journal   = {CoRR},
  volume    = {abs/1802.09477},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.09477},
  archivePrefix = {arXiv},
  eprint    = {1802.09477},
  timestamp = {Fri, 02 Mar 2018 13:46:22 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-09477},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
tangkaratt2018guide,
title={Guide Actor-Critic for Continuous Control},
author={Voot Tangkaratt and Abbas Abdolmaleki and Masashi Sugiyama},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJk59JZ0b},
}

@InProceedings{pmlr-v48-mniha16,
  title =    {Asynchronous Methods for Deep Reinforcement Learning},
  author =   {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  booktitle =    {Proceedings of The 33rd International Conference on Machine Learning},
  pages =    {1928--1937},
  year =     {2016},
  editor =   {Maria Florina Balcan and Kilian Q. Weinberger},
  volume =   {48},
  series =   {Proceedings of Machine Learning Research},
  address =      {New York, New York, USA},
  month =    {20--22 Jun},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v48/mniha16.pdf},
  url =      {http://proceedings.mlr.press/v48/mniha16.html},
}

@incollection{NIPS2017_7112,
title = {Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation},
author = {Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5279--5288},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation.pdf}
}

@article{WangBHMMKF16,
  author    = {Ziyu Wang and
               Victor Bapst and
               Nicolas Heess and
               Volodymyr Mnih and
               R{\'{e}}mi Munos and
               Koray Kavukcuoglu and
               Nando de Freitas},
  title     = {Sample Efficient Actor-Critic with Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1611.01224},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01224},
  archivePrefix = {arXiv},
  eprint    = {1611.01224},
  timestamp = {Tue, 08 Aug 2017 15:06:57 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/WangBHMMKF16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{abs-1801-01290,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
               with a Stochastic Actor},
  journal   = {CoRR},
  volume    = {abs/1801.01290},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.01290},
  archivePrefix = {arXiv},
  eprint    = {1801.01290},
  timestamp = {Thu, 01 Feb 2018 19:52:26 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-01290},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@conference{GuLilGhaTurLev17,
  title = {Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic},
  author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
  booktitle = {Proceedings International Conference on Learning Representations 2017},
  publisher = {OpenReviews.net},
  month = apr,
  year = {2017},
  url = {https://openreview.net/pdf?id=rkE3y85ee},
  month_numeric = {4}
}

@inproceedings{
Gruslys2018,
title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},
author={Audrunas Gruslys and Will Dabney and Mohammad Gheshlaghi Azar and Bilal Piot and Marc Bellemare and Remi Munos},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rkHVZWZAZ},
}

@article{Haarnoja2017,
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
file = {:home/tor/rsc/mendeley/Haarnoja et al/2017/2017 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor - Haarnoja et al. - Unknown.pdf:pdf},
mendeley-groups = {rl-foundation/method/actor-critic},
number = {Nips},
title = {{Soft Actor-Critic : Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
year = {2017}
}

@article{Wang2016,
archivePrefix = {arXiv},
arxivId = {1611.01224},
author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
doi = {10.23915/DISTILL.00001},
eprint = {1611.01224},
file = {:home/tor/rsc/mendeley/Wang et al/2016/2016 - Sample Efficient Actor-Critic with Experience Replay - Wang et al. - Unknown.pdf:pdf},
isbn = {1576853365},
issn = {2476-0757},
mendeley-groups = {rl-foundation/method/actor-critic},
number = {2016},
title = {{Sample Efficient Actor-Critic with Experience Replay}},
url = {http://arxiv.org/abs/1611.01224},
year = {2016}
}

@article{Mnih2016,
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
doi = {10.1177/0956797613514093},
eprint = {1602.01783},
file = {:home/tor/rsc/mendeley/Mnih et al/2016/2016 - Asynchronous Methods for Deep Reinforcement Learning - Mnih et al. - Unknown.pdf:pdf},
isbn = {9781510829008},
issn = {1938-7228},
mendeley-groups = {rl-foundation/method/actor-critic},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}

@ARTICLE{6392457,
author={I. Grondman and L. Busoniu and G. A. D. Lopes and R. Babuska},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
title={A Survey of Actor-Critic Reinforcement Learning: Standard and Natural Policy Gradients},
year={2012},
volume={42},
number={6},
pages={1291-1307},
keywords={function approximation;gradient methods;learning (artificial intelligence);RL;actor-critic reinforcement learning;low-variance gradient estimation;natural policy gradients;optimal policies;policy-gradient-based actor-critic algorithms;real-life applications;standard policy gradients;Approximation algorithms;Approximation methods;Convergence;Equations;Optimization;Standards;Actor-critic;natural gradient;policy gradient;reinforcement learning (RL)},
doi={10.1109/TSMCC.2012.2218595},
ISSN={1094-6977},
month={Nov},}
