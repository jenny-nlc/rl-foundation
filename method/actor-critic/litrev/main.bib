@article{Haarnoja2017,
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
file = {:home/tor/rsc/mendeley/Haarnoja et al/2017/2017 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor - Haarnoja et al. - Unknown.pdf:pdf},
mendeley-groups = {rl-foundation/method/actor-critic},
number = {Nips},
title = {{Soft Actor-Critic : Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
year = {2017}
}

@article{Wang2016,
archivePrefix = {arXiv},
arxivId = {1611.01224},
author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
doi = {10.23915/DISTILL.00001},
eprint = {1611.01224},
file = {:home/tor/rsc/mendeley/Wang et al/2016/2016 - Sample Efficient Actor-Critic with Experience Replay - Wang et al. - Unknown.pdf:pdf},
isbn = {1576853365},
issn = {2476-0757},
mendeley-groups = {rl-foundation/method/actor-critic},
number = {2016},
title = {{Sample Efficient Actor-Critic with Experience Replay}},
url = {http://arxiv.org/abs/1611.01224},
year = {2016}
}

@article{Mnih2016,
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
doi = {10.1177/0956797613514093},
eprint = {1602.01783},
file = {:home/tor/rsc/mendeley/Mnih et al/2016/2016 - Asynchronous Methods for Deep Reinforcement Learning - Mnih et al. - Unknown.pdf:pdf},
isbn = {9781510829008},
issn = {1938-7228},
mendeley-groups = {rl-foundation/method/actor-critic},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}

@article{Gruslys2017,
archivePrefix = {arXiv},
arxivId = {1704.04651},
author = {Gruslys, Audrunas and Azar, Mohammad Gheshlaghi and Bellemare, Marc G. and Munos, Remi},
eprint = {1704.04651},
file = {:home/tor/rsc/mendeley/Gruslys et al/2017/2017 - The Reactor A Sample-Efficient Actor-Critic Architecture - Gruslys et al. - Unknown.pdf:pdf},
mendeley-groups = {rl-foundation/method/actor-critic},
title = {{The Reactor: A Sample-Efficient Actor-Critic Architecture}},
url = {http://arxiv.org/abs/1704.04651},
year = {2017}
}

@ARTICLE{8103164,
author={K. Arulkumaran and M. P. Deisenroth and M. Brundage and A. A. Bharath},
journal={IEEE Signal Processing Magazine},
title={Deep Reinforcement Learning: A Brief Survey},
year={2017},
volume={34},
number={6},
pages={26-38},
keywords={cameras;learning (artificial intelligence);mobile robots;neural nets;artificial intelligence;asynchronous advantage actor critic;autonomous systems;camera inputs;central algorithms;deep Q-network;deep neural networks;deep reinforcement learning;policy-based method;robot control policies;trust region policy optimization;value-based method;Artificial intelligence;Learning (artificial intelligence);Machine learning;Neural networks;Signal processing algorithms;Visualization},
doi={10.1109/MSP.2017.2743240},
ISSN={1053-5888},
month={Nov},}

@ARTICLE{6392457,
author={I. Grondman and L. Busoniu and G. A. D. Lopes and R. Babuska},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
title={A Survey of Actor-Critic Reinforcement Learning: Standard and Natural Policy Gradients},
year={2012},
volume={42},
number={6},
pages={1291-1307},
keywords={function approximation;gradient methods;learning (artificial intelligence);RL;actor-critic reinforcement learning;low-variance gradient estimation;natural policy gradients;optimal policies;policy-gradient-based actor-critic algorithms;real-life applications;standard policy gradients;Approximation algorithms;Approximation methods;Convergence;Equations;Optimization;Standards;Actor-critic;natural gradient;policy gradient;reinforcement learning (RL)},
doi={10.1109/TSMCC.2012.2218595},
ISSN={1094-6977},
month={Nov},}
