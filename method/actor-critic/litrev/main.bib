@inproceedings{
Gruslys2018,
title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},
author={Audrunas Gruslys and Will Dabney and Mohammad Gheshlaghi Azar and Bilal Piot and Marc Bellemare and Remi Munos},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rkHVZWZAZ},
}

@article{Haarnoja2017,
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
file = {:home/tor/rsc/mendeley/Haarnoja et al/2017/2017 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor - Haarnoja et al. - Unknown.pdf:pdf},
mendeley-groups = {rl-foundation/method/actor-critic},
number = {Nips},
title = {{Soft Actor-Critic : Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
year = {2017}
}

@article{Wang2016,
archivePrefix = {arXiv},
arxivId = {1611.01224},
author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
doi = {10.23915/DISTILL.00001},
eprint = {1611.01224},
file = {:home/tor/rsc/mendeley/Wang et al/2016/2016 - Sample Efficient Actor-Critic with Experience Replay - Wang et al. - Unknown.pdf:pdf},
isbn = {1576853365},
issn = {2476-0757},
mendeley-groups = {rl-foundation/method/actor-critic},
number = {2016},
title = {{Sample Efficient Actor-Critic with Experience Replay}},
url = {http://arxiv.org/abs/1611.01224},
year = {2016}
}

@article{Mnih2016,
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
doi = {10.1177/0956797613514093},
eprint = {1602.01783},
file = {:home/tor/rsc/mendeley/Mnih et al/2016/2016 - Asynchronous Methods for Deep Reinforcement Learning - Mnih et al. - Unknown.pdf:pdf},
isbn = {9781510829008},
issn = {1938-7228},
mendeley-groups = {rl-foundation/method/actor-critic},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}

@ARTICLE{6392457,
author={I. Grondman and L. Busoniu and G. A. D. Lopes and R. Babuska},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
title={A Survey of Actor-Critic Reinforcement Learning: Standard and Natural Policy Gradients},
year={2012},
volume={42},
number={6},
pages={1291-1307},
keywords={function approximation;gradient methods;learning (artificial intelligence);RL;actor-critic reinforcement learning;low-variance gradient estimation;natural policy gradients;optimal policies;policy-gradient-based actor-critic algorithms;real-life applications;standard policy gradients;Approximation algorithms;Approximation methods;Convergence;Equations;Optimization;Standards;Actor-critic;natural gradient;policy gradient;reinforcement learning (RL)},
doi={10.1109/TSMCC.2012.2218595},
ISSN={1094-6977},
month={Nov},}
