\section{Architecture}
% Identify components
% * psi: the advantage that is put in gradient
% * optimization method: adam, rmsprop, sgd, kfac
% * natural vs standard grad, see survey

% To ensure exploration in the policy
% * to use entropy regularization to ensure exploration in the policy,
% which is a common practice in policy gradient (Williams & Peng, 1991; Mnih et al., 2016).
% * to use KL-divergence as a constraint on how much deviation is permitted from a prior policy
% (Bagnell & Schneider, 2003; Peters et al., 2010; Schulman et al., 2015; Fox et al., 2015).

The objective is to
maximize the expected $\gamma$-discounted cumulative return,
$J(\theta) = \mathbb{E}_{\pi} [R_t] = \mathbb{E}_{\pi} [ \sum_{i \ge 0}^{\infty} \gamma^i r(s_{t+i}, a_{t+1}) ]$
with respect to the policy parameters~$\theta$.
Note that $J(\theta)$ is a nonconvex function.

The policy gradient is defined as:
$\nabla_{\theta} J(\theta) = \mathbb{E}_{\pi} [ \sum_{i \ge 0}^{\infty} \Psi^t \nabla log \pi_{\theta} (a_t | s_t) ]$,
where $\Psi^t$ is often chosen to be the advantage function $A^{\pi}(s_t,a_t)$,
which provides a relative measure of valur of each action~$a_t$ at a given state~$s_t$.
