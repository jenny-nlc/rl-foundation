# Asynchronous Methods for Deep Reinforcement Learning
33rd International Conference on Machine Learning, New York, NY, USA, 2016

## problem
* Deep RL algorithms based on experience replay; drawbacks: 
  * it uses more memory and computation per real interaction; and 
  * it requires off-policy learning algorithms that can update from data generated by an older policy.

## idea: A3C
* framework for deep reinforcement learning that uses asynchronous gradient descent for
  optimization of deep neural network controllers.
* Instead of experience replay, we asynchronously execute multiple agents in parallel, 
  on multiple instances of the environment. 
  * This parallelism decorrelates the agents’ data into a more stationary process, since 
    at any given time-step the parallel agents will be experiencing a variety of different states.

## setup
* task: Atari domain, exploring 3D mazes purely from visual inputs

## result
* The best performing method, **an asynchronous variant of actor-critic**, surpasses
  the current state-of-the-art on the Atari domain while training for
  half the time on **a single multi-core CPU** instead of a GPU.
* using parallel actorlearners to update a shared model had
  a stabilizing effect on the learning process of the three value-based methods

## misc
* unstability of deepNN on RL caused by
  * the sequence of observed data encountered by an online RL agent is non-stationary, and 
  * online RL updates are strongly correlated.
* approaches to stabilize deep neural networks on online RL algorithms:
  * by storing the agent’s data in an experience replay memory, 
    the data can be batched (Riedmiller, 2005; Schulman et al., 2015a) or 
    randomly sampled (Mnih et al., 2013; 2015; Van Hasselt et al., 2015) from different time-steps. 
  * Aggregating over memory in this way reduces non-stationarity and decorrelates updates, but 
    at the same time limits the methods to off-policy reinforcement learning algorithms.
