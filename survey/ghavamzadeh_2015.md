# Bayesian Reinforcement Learning: A Survey
* Foundations and TrendsR ? in Machine Learning
* Vol. 8, No. 5-6 (2015) 359–483
* M. Ghavamzadeh, S. Mannor, J. Pineau, and A. Tamar
* DOI: 10.1561/2200000049

## abstract
* Bayesian methods
  * for incorporating prior information into inference algorithms.
* incentives for incorporating Bayesian reasoning in RL are:
  * provides an elegant approach to action-selection (explo- ration/exploitation)
    as a function of the uncertainty in learning; and
  * provides a machinery to incorporate prior knowledge into the algorithms
  * implicitly facilitates regularization
    * The role of the prior is therefore to
      soften the effect of sampling a finite dataset, effectively leading to regularization.
  * the principled Bayesian approach for handling parameter uncertainty

## 1: intro
* rl vs supervised learning (sl):
  * sl:
    * deal with independently and iden- tically distributed (i.i.d.) samples from the domain
    * data are given, no policy for collecting data, but cf active learning
  * rl:
    * learns from the samples that are collected from the trajectories generated by
      its sequential interaction with the system.
    * different policies naturally yield different distributions of sampled trajectories,
      and thus, impacting what can be learned from the data.
* Bayesian reinforcement learning (BRL)
  * is an approach to RL that leverages methods from Bayesian inference to
    incorporate information into the learning process.
  * assumes that
    * the designer of the system can express prior information about
      the problem in a probabilistic distri- bution, and
    * new information can be incorporated using standard rules of Bayesian inference.
  * for model-based:
  * The information can be encoded and updated using a parametric representation
    * of the system dynamics, in the case of model-based RL, or
    * of the solution space, in the case of model-free RL
* challenges arise in applying Bayesian to RL paradigm.
  * there is the challenge of selecting the correct representation for
    expressing prior information in any given domain.
  * defining the decision-making process over the information state is
    typically computationally more demanding than
    directly considering the natural state representation

## 5: model-free bayesian reinforcement learning
TODO

## 8: outlook
TODO


<!--
Traditionally, RL algorithms have been categorized as being ei-
ther model-based or model-free. In the former category, the agent uses the collected data to first build a model of the domain’s dynamics and then uses this model to optimize its policy. In the latter case, the agent directly learns an optimal (or good) action-selection strat- egy from the collected data. There is some evidence that the first method provides better results with less data [Atkeson and Santa- maria, 1997], and the second method may be more efficient in cases where the solution space (e.g., policy space) exhibits more regular- ity than the underlying dynamics, though there is some disagreement about this,.
 -->
