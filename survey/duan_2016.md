# Benchmarking Deep Reinforcement Learning for Continuous Control

## Task
* Basic (5):
  * Cart-Pole Balancing (Stephenson, 1908; Donaldson, 1960; Widrow, 1964; Michie & Chambers, 1968), 
  * Cart-Pole Swing Up (Kimura & Kobayashi, 1999; Doya, 2000), 
  * Mountain Car (Moore, 1990), 
  * Acrobot Swing Up (DeJong &Spong, 1994; Murray&Hauser, 1991; Doya, 2000), and 
  * Double Inverted Pendulum Balancing (Furuta et al., 1978).
* Locomotion (6):
  * Swimmer (Purcell, 1977; Coulom, 2002; Levine & Koltun, 2013; Schulman et al., 2015a), 
  * Hopper (Murthy & Raibert, 1984; Erez et al., 2011; Levine & Koltun, 2013; Schulman et al., 2015a), 
  * Walker (Raibert&Hodgins, 1991; Erez et al., 2011; Levine & Koltun, 2013; Schulman et al., 2015a), 
  * Half-Cheetah (Wawrzynski, 2007; Heess et al., 2015b), 
  * Ant (Schulman et al., 2015b), Simple Humanoid (Tassa et al., 2012; Schul- man et al., 2015b), and 
  * Full Humanoid (Tassa et al., 2012).
* Partially Observable (on basic tasks: 3 x 5):
  * Limited Sensors
  * Noisy Observations and Delayed Actions
  * System Identification
* Hierarchical
  * Locomotion + Food Collection
  * Locomotion + Maze
  
