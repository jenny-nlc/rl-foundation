# 9: On-policy Prediction with Approximation

* fn approximator:
  * to approx `v_{\pi}` from experience generated using a known policy `\pi`
  * `\hat{v}_{\pi}`is represented as a parameterized functional form with weight vector `w in R^d`;
    **not** as a table anymore
  * `\hat{v}(s,w) \approx v_{\pi}(s)`
  * `\hat{v}` can be:
    * a linear function in features of the state,
      with `w` the vector of feature weights.
    * a multi-layer artificial neural network,
      with `w` the vector of connection weights in all the layers.
    * a decision tree,
      where `w` is all the numbers defining the split points and leaf values of the tree.
  * the number of weights (the dimensionality of w) is **much less** than the number of states,
    `(d << |S|)`
* generalization
  * changing one weight changes the estimated value of many states
  * when a single state is updated,
    the change generalizes from that state to affect the values of many other states.
* function approximation makes RL applicable to **partially observable** problems,
  * If the parameterized function form for `\hat{v}` does **not allow**
    the estimated value to depend on certain aspects of the state, then
    it is just **as if** those aspects are unobservable.
* function approximation can **not augment**
  the state representation with **memories of past observations**

## 9.1 Value-function Approximation
* updates `s \mapsto u`
  * `s`: the (arbitrary) state updated,
      * cf `S_t`: state encountered in actual experience
  * `u`: the update target that `s`'s estimated value is shifted toward.
  * natural to interpret each update as
    specifying an example of the desired input-output behavior of the value function;
    supervised learning methods
    * `s \mapsto g` of each update as a **training example**
  * types:
    * Monte Carlo update: `S_t \mapsto G_t`
    * TD(0): `S_t \mapsto R_{t+1} + \gamma \hat{v}(S_{t+1},w_t)`
    * n-step TD: `S_t \mapsto G_{t:t+n}`
    * DP: `s \mapsto E_{\pi}[  R_{t+1} + \gamma \hat{v}(S_{t+1},w_t) | S_t = s  ]`
* On applying supervised learning in RL
  * able to occur online, while the agent interacts with its environment or
    with a model of its environment.
    * requires methods that are able to learn efficiently from incrementally acquired data
  * able to handle nonstationary target functions (target functions that change over time).
    * For example, in control methods based on GPI (generalized policy iteration)
      we often seek to learn `q_{\pi}` while `\pi` changes.
    * Even if the policy remains the same, the target values of training examples are
      nonstationary if they are generated by bootstrapping methods (DP and TD learning)

## 9.2 The Prediction Objective
* a natural objective function, the Mean Squared Value Error, `\bar{VE}`
  * `\bar{VE} = \sum_{s \in S} \mu(s) [v_{\pi(s)} - \hat{v}(s,w)]^2`
    * `\mu(s) \ge 0` and `\sum_s \mu(s) = 1`, specify state distribution
  * note: The best value function for better policy is
    **not necessarily** the best for minimizing `\bar{VE}`
* `\mu(s)`
  * is often chosen to be the fraction of time spent in s.
  * Under on-policy training this is called the on-policy distribution
  * The on-policy distribution is
    the fraction of time spent in each state normalized to sum to one
    * `\mu(s) = \frac{\nu(s)}{\sum_{s'} \nu(s')}`
      * `\nu(s)`: the number of time steps spent, on average, in state `s` in a single episode

## 9.3 Stochastic-gradient and Semi-gradient Methods
TODO
