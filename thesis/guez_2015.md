# Arthur Guez: Sample-based search methods for bayes-adaptive planning, 2015

## abs
A model-based Bayesian agent optimizes its return by main-
taining a posterior distribution over possible environments, and considering all
possible future paths. This optimization is equivalent to solving a Markov Deci-
sion Process (MDP) whose hyperstate comprises the agent’s beliefs about the
environment, as well as its current state in that environment. This corresponding
process is called a Bayes-Adaptive MDP (BAMDP).

Our algorithms are sample-based, plan online in a way that is focused
on the current belief, and, critically, avoid expensive belief updates during simu-
lations. In discrete domains, we use Monte-Carlo tree search to search forward
in an aggressive manner.

Bayes-Adaptive Monte-Carlo Planning (BAMCP)

## discussion
BAMCP: to employ Monte-Carlo tree search to explore the augmented Bayes-adaptive search space efficiently.

three modifications to obtain a computationally tractable sample-based algorithm:
* root sampling, which only requires beliefs to be sampled at the start of each simulation
* a model-free RL algorithm that learns a rollout policy
* a lazy sampling scheme that enables the posterior beliefs to be sampled cheaply

BAFA subsumes various existing tree-based approaches such as BAMCP.
It allows the values accorded to nearby belief-states to be shared –
each simulation is no longer an isolated path in an exponentially growing tree,
but can impact many non-visited beliefs and states.
