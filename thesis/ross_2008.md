# Model-based Bayesian Reinforcement Learning in complex domains
ross_2013 (msc thesis)

Ross~\cite{Ross2008msc} points out that learning mechanisms are necessary to improve the model from previous experience
in order to improve the agent's decision in the future.
Ross in 2008 proposed BAPOMDP, which is an extension of model-based bayesian reinforcement learning to partially observable domains.
He also introduced Bayes-Adaptive Continuous POMDP model.
Lastly, he extended model-based bayesian reinforcement learning to structured domains.

Key to the proposed solutions include:
particle filter algorithms and an online planning algorithm,
an appropriate choice of posterior distribution and Monte Carlo methods.

An MDP is defined by 5 components, i.e. $(S,A,T,R,\lambda)$,
where $T: S \times A \times S \mapsto [0,1]$.
The definition of $T$ is based on the Markov assumption that assumes that
the next state depends only on the current state and action.
Also, $T$ is stationary or time-homogenous, i.e it does not vary over time.

A more general definition of a policy, $\phi: S \times A \mapsto [0,1]$, i.e.
a mapping that specifies the probability $\phi(s,a) = Pr(a_t = a | s_t = s)$.
A deterministic policy is a policy that assigns, for every state, a probability of 1 to a particular action.

The goal of the agent is to find the optimal policy $\phi^{*}$ that
maximizes its expected return starting from the initial state $s_0$, $\phi^{*} = argmax_{\phi \in \Pi} V^{\phi}(s_0)$.
Approaches to solve MDP include value-iteration and sparse Monte Carlo based sampling.

A POMDP is defined formally by seven components $(S, A, Z, T, O, R, \lambda)$, where
$Z$ is the set of observations the agent can perceive in its environment.
Observation probability $O: S \times A \times Z \mapsto [0,1]$ specifies the probability
$P(z_t = z | s_t = s', a_{t-1} = a)$ that the agents receives an observation $z$ when it moves to $s'$ by doing~$a$.
It is assumed that $O$ is time-invariant or stationary or time-homogenous and follows Markov assumption.
POMDP is equivalent to belief MDP.
Approches to solve POMDP planning can be divided into: exact, point-based and online approaches.

Model-free methods try to learn the optimal policy directly, without learning the transition and immediate reward model,
while model-based methods try to estimate the transition and immediate reward model, in order to compute the optimal policy afterwards.
Heuristics to balance the exploration and exploitation include $\epsilon$-greedy and Boltzmann exploration, interval estimation,
$E^3$ algorithm (Explicit Explore or Exploit), R-Max

Ross extends the work of Duff on BAMDP.
One main challenge is how to update the Dirichlet count parameters when the states are hidden (partially observable).
